## Quality of Activity Prediction

### Overview
The purpose of this project was to develop a model that can predict the class of exercise activity performed by a subject. The data is from Velloso et al. [1], and consists of sensor readings of subjects performing dumbbell bicepts curls correctly and incorrectly. There are 5 classes of exercise: correct and 4 different types of incorrect form. The model was created using training data supplied for the course and was tested on 20 held out records in a final test set. The following sections describe the choices made in setting up and selecting a model, how the final model was built, the out-of-sample error estimates, and the final prediction on the test dataset.

```{r}
# Gather all data from training set
trainAllDf <- read.csv("pml-training.csv",stringsAsFactors=FALSE, header=TRUE, na.strings=c("","NA"))

dim(trainAllDf)
# Just show part of the features so the output is not too large
trainAllDf[1:3,1:50]

# Read in data from the final test set
testDf <- read.csv("pml-testing.csv",stringsAsFactors=FALSE, header=TRUE, na.strings=c("","NA"))
```

###Choices and Rationale

A review of the training data showed that the columns with processed data (average, max, etc.) consisted of NAs. A sample is shown above. These columns were simply eliminated in both the testing and training datasets.

In addition, the first 5 features seemed to be identifiers and timestamps related to the experiment itself and not to the actual data collected (the movements as recorded by the sensors). Consequently, the first 5 variables were eliminated from both training and testing data.

According to Velloso et al., class A indicates performing the activity correctly. Classes B-E each indicate a specific exercise mistake. However, since one type of error does not have more or less importance than another error, and since each run during cross validation uses the same number of instances, it was decided to simply count the number of incorrect predictions as a measure of the out of sample error. This count was used to determine an average error count during cross validation. Also, the percentage these misclassifications made out the the total number of predictions was calculated.

Velloso et al. used a random forest in their analysis. In addition, because random forests automatically do ensemble prediction, which result in accurate classifiers, it was decided to try random forests for this project.

Because the random forest method randomly choses a subset of features to build trees [2], and hence chooses different features each time, it was decided to let the algorithm perform feature selection. With a large number of trees, many combinations of features should be generated, and it would be interesting to see if the resulting model can make accurate predictions without a human doing feature selection.

Also, although Velloso et al. determined that the 2.5 window gave the best results, it was decided to let the random forest work on all data rows, to see how well it did.

### Data Cleanup
Columns with these prefixes have NA's, and are removed: 
kurtosis_,
skewness_,
max_,
min_,
amplitude_,
var_,
avg_,
stddev_.

Also the first several columns are things like id, user_name -- remove these.
```{r}
cleanUp <- function(inDf) {
  features <- names(inDf)
  dontWant <- grep("^kurtosis_|^skewness|^max_|^min_|^amplitude_|^var_|^avg_|^stddev_", features, perl=TRUE, value=FALSE)
  dontWant <- c(dontWant, 1,2,3,4,5)
  outDf <- inDf[, -dontWant]
  return (outDf)
}
```
Clean up both training and testing the same way, except the trainAllDF has the classe column and the testDf does not.

Also, need to make new_window in the testDf have 2 levels to match the number of levels in trainAllDf. And change classe to a factor.
```{r}
trainAllDf <- cleanUp(trainAllDf)
testDf <- cleanUp(testDf)
trainAllDf$classe <- as.factor(trainAllDf$classe)
trainAllDf$new_window <- as.factor(trainAllDf$new_window)
testDf$new_window <-factor(testDf$new_window, levels=c("no", "yes"))
dim(trainAllDf)
```

### Setting up for Model Building 
During the cross-validation step, training data will be split into sub-testing and sub-training sets to generate error estimates. In addition, to get an error estimate on data not used to build the model, another set was separated from the trainAllDf and held in reserve. For lack of a better term (since the final test data is the "test set"), this set is called a "validation"" set in this project.

Datasets:

1. trainAllDf -- split into validDf and trainDf.
2. trainDf -- split into sub-training and sub-testing sets during cross validation.
3. validDf -- the "validation" set.
4. testDf -- the final test set.

```{r}
# Get ready to create the model
library(randomForest)
set.seed(1131)
# Split up trainAllDf
idx <- sample(nrow(trainAllDf), 0.8*nrow(trainAllDf))
trainDf <- trainAllDf[idx,] # set for cross validation runs
validDf <- trainAllDf[-idx, ] # for more accurate out-of-sample err estimate
```

### Cross Validation
Here, define a cross-validation function for running the random forest. Caret proved to be too slow to be practical.

```{r}
# Function to do the cross validation
run_model <- function(dframe, num_trees, mtry_val) {
  # Run this particular model 10 times
  nfolds <- 10
  foldRes <- vector(length=nfolds)
  folds <- 1:nfolds
  for (fold in 1:nfolds) {
    # Use a random 80% training/20% testing split
    idx <- sample(nrow(trainDf), 0.8*nrow(trainDf))
    train <- dframe[idx,]
    test <- dframe[-idx, ]
    rf <- randomForest(classe~., data=train, ntree=num_trees, 
                       mtry=mtry_val, proximity=FALSE)
    pred <- predict(rf, newdata=test)
    confusion<-table(pred, test$classe)
    # Get the diagonals -- these are correct, and subtract from
    # the total number of test rows to get the number misclassified.
    # Save this for each of the 10.
    foldRes[fold] <- nrow(test)-sum(confusion[seq(1,25,6)])
    #print(paste("Fold:", fold, ". Err:", foldRes[fold]))
  }

  # Now that we have the 10 runs for this model, get its err information
  avgErr = mean(foldRes) # avg wrong for this ntree and mtry
  sdErr = sd(foldRes)
  #print(paste("Average Err", avgErr, "Stddev:", sdErr))
  ret_list <- list(avg=avgErr, stddev=sdErr)
  return (ret_list)
}
```
### Model Building
According to the randomForest description on CRAN [2], the number of trees "should not be set to too small a number, to ensure that every input row gets predicted at least a few times." Velloso et al. built their random forest with only 10 trees, which seems too small. In addition, for the mtry parameter, the default number of variables to sample for the candidate trees is the sqrt(number of features). After pruning, sqrt(num features) here is about 7. 

Consequently, several tree sizes (parameter ntree) and several variable sizes (parameter mtry) were put through cross validation. To select the final model to use with the test dataset, the decision was to use the ntree and mtry values that resulted in the lowest misclassification in the cross validation step.

```{r}
# Variables to capture the best seen
bestErr = 100.0
bestSd = 0.0
bestM = 0
bestT = 0
# Try several tree sizes and numbers of variables (mtry).
# Try somewhat small to somewhat large number of trees.
# Try values below and above sqrt(54) for mtry.
# Put each combination through the cross-validation step on the trainDf data
# This will take a long time to run.
for (t in c(50, 100, 200, 400)) {
  for (m in c(4, 8, 12, 16, 20)) {
    res_rf <- run_model(trainDf, t, m)
    print(paste("Num trees:", t, "mtry:", m, "- result avg error count:", 
                round(res_rf$avg, 3), "stddev:", round(res_rf$stddev,3)))
    if (res_rf$avg < bestErr) {
      bestErr <- res_rf$avg
      bestSd <- res_rf$stddev
      bestM <- m
      bestT <- t
    }
  }
}
```

### Final Model and Out of Sample Error Estimation
Cross validation found the following random forest model as the best (smallest missclassification). In addition, the average misclassification count for that particular cross validation run (which fit 10 models) is given as the out of sample error estimate. This best model is refit here (since it was not saved in the run_model function).

```{r}
print(paste("Best ntree:", bestT, " Best mtry:", bestM, 
        "- Out of Sample Error Estimate (the average misclassified count):",
        bestErr, " Stddev:", round(bestSd, 3)))
print(paste(" Misclassification as percent of total instances:",
        round(100*bestErr/(nrow(trainDf)*0.8), 4), "%"))

rfFit<-randomForest(classe~., data=trainDf, ntree=bestT, mtry=bestM,
                    proximity=FALSE)
```

The saved validation data is used to get a better out of sample error estimate:

```{r}
rpred <-predict(rfFit, newdata=validDf)
confusion<-table(rpred, validDf$classe)
validErr <- nrow(validDf)-sum(confusion[seq(1,25,6)])
print(paste("Out of sample error estimate: Misclassification count on validation set:", 
            validErr, " Percent of total instances:", 
            round(100* validErr/nrow(validDf), 3), "%"))
```

### Test Set Prediction
The final step is to do the test prediction and output the results.

```{r}
testPred <-predict(rfFit, newdata=testDf)
testPred

```

Function to output the data:
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

```

Write the files:

```{r}
answers<-as.character(testPred)
pml_write_files(answers)
```

### Wrap Up
Note that this section was added after running the R markdown file initially and generating the predictions. The prediction files were submitted and were 100% correct. Given that the out-of-sample error rate calculated both from cross validation and from the "validation" set was less than 1%, it was expected that the success rate would be very high, close to 100%, and it was.

It was also noted that the standard deviations for all the models were such that it looks like there might not be a significant difference between many (or any of them). Consequently, a second model is fitted below based on only 50 trees with mtry still equal to the best (16) to see if a model that can train faster is as good as the one that was used for the submitted predictions. Being able to achieve the same results with less computation would make the model more useful in a setting with many more data rows in the training set.


```{r}
rfFit2<-randomForest(classe~., data=trainDf, ntree=50, mtry=16,
                    proximity=FALSE)
rpred2 <-predict(rfFit2, newdata=validDf)
confusion<-table(rpred2, validDf$classe)
validErr <- nrow(validDf)-sum(confusion[seq(1,25,6)])
print(paste("Faster model: Misclassification count on validation set:", 
            validErr, " Percent of total instances:", 
            round(100* validErr/nrow(validDf), 3), "%"))

testPred2 <-predict(rfFit2, newdata=testDf)
testPred2

# See if testpred2 == testpred
# the number of true's should sum to length if the two are the same
numSame <- sum(testPred == testPred2)
if (numSame == length(testPred)) {
  print(paste("The faster-to-build model gave the same prediction. Consider using this one in future."))
} else {
  print(paste("The faster-to-build model did not give the same prediction. Keep the original model."))
}
```

### References

[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

[2] Package 'randomForest', CRAN Repository, http://cran.r-project.org/web/packages/randomForest/randomForest.pdf, January 2015.

